******************************COMPREHENSION DU PIPELINE DE TRAINING/DETECTION******************************   

i)	Détection des plans : extraction de toutes les surfaces planes situées face à la caméra (framework déja existant : ORK) > puis extraction du plan le plus proche et horizontale (utilsation d'un second filtre)
        >la frame du plan le plus proche est retournée permettant de connaitre la position de la table par rapport au robot.
        Note: Pour que la detection de la table et des objets fonctionne correctement, il faut que la table soit située bien en face de la caméra, au 1er plan.
        
ii)	 Extraction des objets situés au dessus du plan sous forme de cluster (Pour l’instant objets situés a x cm au dessus du plan)

iii) Utilisation des clusters pour extraire l'image d'un l'objet (à partir de l'image rgb retournée par la caméra et des coordonnées des clusters)

iv)	Phase de training (cf quick start guide pour les noeuds ros à lancer): Attention peut s'effectuer via " call service /object_srv train " ou selon le "rosrun object_discrimination Objectdiscrimination " +  rostopic pub discrim_status object_discrimination/DetectionInput "{'mode':'new_db_objects','label':['bean','can' etc..]}"
a.	remplissage de la bdd  (dossier db)
    b.	un dossier est créé pour chaque objet de la table 
    c.	dossier contient:
        i.	 image rgb de l'objet (cluster.png)
        ii.	 clusters sous forme brut (cluster.pcd et cluster.txt)
        iii.	 info.txt (contient: label, catégorie et autre information lié à l'objet)

v)	Phase de détection (cf quick start guide pour les noeuds ros à lancer)::
    1ère possibilité:
            a.	On indique via un topic le label d'un objet que l'on veut identifier ainsi qu'une catégorie (optionel)
                > rostopic pub discrim_status object_discrimination/DetectionInput "{'mode':'detect','label':['*label*']}"
            b.	Si le label correspond au label d’un objet stocké dans la bdd alors le processus de comparaison entre l’image issue de la bdd et            les images courantes des objets situés sur la tabletop est lancé (méthode de matching : surf …)
            c.	Une information de présence ou non de l’objet demandé sur la tabletop est ensuite retourné
            d.	Info retour ? sous quelle forme ?
     2ème possibilité:                
            a.	Demande au software ce qu’il y a sur la table (reviens à mettre 'all' dans label')
                > rostopic pub discrim_status object_discrimination/DetectionInput "{'mode':'detect','label':['all']}"
            b.	Extraction des clusters correspondants aux  objets sur la table
            c.	Comparaison des clusters avec les clusters des objets stockés en bdd afin de filtrer les objets non ressemblant avant de passer à la comparaison des images rgb
            d.	Si match entre cluster alors les étapes suivantes sont réalisées
            e.	Extraction de l’image rgb des objets situés sur la table
            f.	Comparaison des images extraites avec images de la base de donnée (méthode de matching : surf …)
            g.	Si concordance entre image alors l’objet est connu par le soft
            h.	Donc possibilité de donné le nom des objets situés sur la table
            
******************************A corriger/Améliorer******************************
!!!!!!!!!! Objectdiscrimination > discrimination_manager() > si plus de 1 label et 1 seule catégorie alors erreur (peut etre d'autres cas d'erreur)
!!!! dans FileSystemDao.cpp >> DB_DIRECTORY = chemin absolu et non relatif
!!!! remttre DaoMng et object_extract dans launch
!!!! améliorer affichage detection (dans Objectdiscrimination.cpp)
!!!! si prblm penser à mettre caméra bien en face jusqu'a 
!!!! créer procédure démo (feuille blanche position objets sur table, prosition précise de la caméra % à la table ...)
Lister points à amérliorer et pblm ...!
faire schéma des classes (et expliquer ce que chacune fait)
remplir bdd avec bcp d'élement pour regarder temps de calcul
i) amélioration méthode matching image rgb ?
ii) amélioration méthode de comparaison des clusters
iii) Remplacer texte en dur de info.txt en JSON  (dans FileSystemDao.cpp > saveAllObject)
+ mettre en place parsage JSON
iv)	Multi-threading pour optimisation processus de filtrage bdd
v) améliorer robustesse
vi) changer chemins absolus en chemins relatifs
vii) script python qui modifie nom dossier db à partir de info.txt
viii) Modification phase d'apprentissage : laisser la possibilité au user de nommer directement un objet détecté (pour eviter d'avoir a changer le label dans info.txt)
IX) Modification de la classe Objectdiscrimination: il y a un mode "new_objects_db" qui remplace le mode train amélioré précédement. (A voir)

****************************** En cours ******************************
i) Amélioration training > possibilité de donner le label et la category d'un objet lors du training (seulement si l'objet est seul sur la table)
       Commande ros : rosservice call /train_one_object_srv *label* *category* 
       Problème à prendre en compte: si l'utilisateur ne voit qu'un objet alors que la caméra en detecte 2 => erreur
       !!! sizeof() !=1 > pas bon, a modifier > on ne test pas si il y a un seul cluster!!!
       Problème corrigé.
       

ii) réfléchir autre caractéristique de comparaison des clusters
iii) mettre en place new methodes de fitting

****************************** Nos modifications ******************************
i) Correction erreur detection avec rostopic pub ..."{'mode':'detect','label':['all']}" > gestion du cas ou aucune catégorie n'est spécifiée
        > classe modifiée : Objectdiscrimination

ii) JSON ok (à compléter - qu'est ce qui a été fait + classes modifiées)

iii) Projet sur PC portable ok

iv) Phase training (parler du training pour un objet)




